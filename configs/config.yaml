# -----MODEL-----
d_mlp: 256
d_model: 64

# Whether to attempt to use flash attention.
# torch will gracefully handle cases where required
# resources are unavailble
flash_attention: True
mask_type: causal
n_blocks: 6
n_ctx: 32
n_heads: 8
name: dtt
seed: 91

# -----TOKENIZER-----
vocab_size: 16384

# -----TRAINING-----
batch_size: 32
epochs: 1
grad_accumulation_steps: 1
dropout: 0.2

# -----DATASET-----
dataset: wiki
overlap: 4
n_workers: 2

# Min allowed ratio between smallest and largest number of shards
# allocated to a specific rank process before we utilise a shared
# shard strategy. See `dtt.utils.get_dataloader` for details.
min_loader_ratio: 0.85 

# -----RESOURCES-----
cuda: True
autocast: True

# -----OPTIMIZER-----
optimizer:
  name: adamw
  params:
    weight_decay: 0.0001
    fused: True

# -----LR Schedule-----
lr_schedule:
  name: onecycle
  params:
    lr_max: 0.01
    pct_start: 0.1 # Percentage of training to increase lr for 

# -----VALIDATION----- 
log_per_val: 1000
temp: 1
val_prompt: Hello, my name is dtt. DTT stands for

# -----LOGGING-----
wandb: False
eff_batch_per_log: 100
profile: False

# -----PATH TEMPLATES-----
templates: configs/path_templates.yaml
