# -----MODEL-----
d_mlp: 3072
d_model: 768

# Whether to attempt to use flash attention.
# torch will gracefully handle cases where required
# resources are unavailble
flash_attention: True
mask_type: causal
n_blocks: 12
n_ctx: 1024
n_heads: 12
name: dtt
seed: 91

# -----TOKENIZER-----
vocab_size: 49152

# -----TRAINING-----
batch_size: 64
epochs: 1
grad_accumulation_steps: 1
dropout: 0.0

# -----DATASET-----
dataset: wiki
overlap: 16
n_workers: 2

# Min allowed ratio between smallest and largest number of shards
# allocated to a specific rank process before we utilise a shared
# shard strategy. See `dtt.utils.get_dataloader` for details.
min_loader_ratio: 0.85 

# -----RESOURCES-----
cuda: True
autocast: True

# -----OPTIMIZER-----
optimizer:
  name: adamw
  params:
    weight_decay: 0.01
    fused: True

# -----LR Schedule-----
lr_schedule:
  name: onecycle
  params:
    lr_max: 0.0006
    pct_start: 0.04 # Percentage of training to increase lr for 

# -----VALIDATION----- 
log_per_val: 100
temp: 1
val_prompt: Hello, my name is dtt. DTT stands for

# -----LOGGING-----
wandb: False
eff_batch_per_log: 50
profile: False

# -----PATH TEMPLATES-----
templates: configs/path_templates.yaml
