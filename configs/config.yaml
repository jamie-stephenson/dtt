# -----MODEL-----
d_mlp: 32
d_model: 16
mask_type: causal
n_blocks: 4
n_ctx: 16
n_heads: 8
name: jet
seed: 91

# -----TOKENIZER-----
vocab_size: 2048

# -----TRAINING-----
batch_size: 16
epochs: 1
grad_accumulation_steps: 1
dropout: 0.2

# -----DATASET-----
dataset: wiki
overlap: 4
n_workers: 0

# -----RESOURCES-----
cuda: False
autocast: False

# -----OPTIMIZER-----
optimizer:
  name: adamw
  params:
    weight_decay: 0.0001
    fused: False

# -----LR Schedule-----
lr_schedule:
  name: onecycle
  params:
    lr_max: 0.01
    pct_start: 0.1 # Percentage of training to increase lr for 

# -----VALIDATION----- 
log_per_val: 10
temp: 1
val_prompt: Hello, my name is Jet. J.E.T. stands for

# -----LOGGING-----
no_wandb: True
eff_batch_per_log: 100

# -----PATH TEMPLATES-----
templates: configs/path_templates.yaml
